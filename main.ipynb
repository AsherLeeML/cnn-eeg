{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models\n",
    "import nndata\n",
    "reload(nndata)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic crossvalidation\n",
    "\n",
    "A basic performance crossvalidation for all subsets of the data with 2, 3, or 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "SPLITS = 5\n",
    "input_length = 3 * 160 # = 3s\n",
    "electrodes = range(64)\n",
    "epochs = 3\n",
    "epoch_steps = 5 # record performance 5 times per epoch\n",
    "batch = 32\n",
    "nclasses = [2, 3, 4]\n",
    "splits = range(5)\n",
    "\n",
    "results = np.zeros((len(nclasses), len(splits), 4, epochs*epoch_steps))\n",
    "for j,nclasses in enumerate(nclasses):\n",
    "    try:\n",
    "        del X,y\n",
    "    except:\n",
    "        pass\n",
    "    X,y = nndata.load_raw_data(electrodes=electrodes, num_classes=nclasses)\n",
    "    \n",
    "    steps_per_epoch = np.prod(X.shape[:2]) / batch * (1-1./SPLITS) / epoch_steps\n",
    "    for ii,i in enumerate(splits):\n",
    "        print \"%d CLASS, SPLIT %d\" % (nclasses, i)\n",
    "        idx = range(len(X))\n",
    "        train_idx, test_idx = nndata.split_idx(i, 5, idx)\n",
    "\n",
    "        model = models.create_raw_model(\n",
    "            nchan=len(electrodes), \n",
    "            nclasses=nclasses, \n",
    "            trial_length=input_length\n",
    "        )        \n",
    "        \n",
    "        # save best weights for each model\n",
    "        weights_path = \"weights-%dcl-%d.hdf5\" % (nclasses,i)\n",
    "        checkpoint = [ModelCheckpoint(filepath=weights_path, save_best_only=True)]\n",
    "        \n",
    "        # run training\n",
    "        h = models.fit_model(\n",
    "            model, X, y, train_idx, test_idx, input_length=input_length, \n",
    "            batch_size=batch,  steps_per_epoch=steps_per_epoch, epochs=epochs*epoch_steps, \n",
    "            callbacks=checkpoint\n",
    "        )\n",
    "\n",
    "        # save training history\n",
    "        results[j, ii, :, :] = [ \n",
    "            h.history[\"acc\"], \n",
    "            h.history[\"loss\"], \n",
    "            h.history[\"val_acc\"], \n",
    "            h.history[\"val_loss\"] \n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject retraining\n",
    "\n",
    "Using pretrained weights, this procedure loops over all subjects and refines the parameters for each subject recording the new accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = 5\n",
    "\n",
    "input_length = 3*160\n",
    "electrodes = range(64)\n",
    "classes = 2\n",
    "\n",
    "X, y = nndata.load_raw_data(electrodes=electrodes, num_classes=classes)\n",
    "\n",
    "weights_file = \"weights-2cl-%d.hdf5\"\n",
    "model = models.create_raw_model(\n",
    "    nchan=len(electrodes), \n",
    "    nclasses=classes, \n",
    "    trial_length=input_length\n",
    ")\n",
    "\n",
    "results = np.zeros((len(X), 2))\n",
    "\n",
    "for split in range(5):\n",
    "    idx = range(len(X))\n",
    "    train_idx, test_idx = nndata.split_idx(split, SPLITS, idx)\n",
    "    Xsub, ysub = nndata.crossval_test(X, y, test_idx, input_length, flatten=False)\n",
    "    \n",
    "    # for each subject in the training set\n",
    "    for i, subject_X in enumerate(Xsub):\n",
    "        current_subject = test_idx[i]\n",
    "        subject_y = ysub[i,:]\n",
    "        print current_subject,\n",
    "        \n",
    "        # accuracy without retraining\n",
    "        model.load_weights(weights_file % (split))\n",
    "        results[current_subject, 0] = model.evaluate(\n",
    "            subject_X.reshape((-1,) + (input_length, len(electrodes), 1)), \n",
    "            subject_y.reshape((-1, classes)), verbose=0\n",
    "        )[1]\n",
    "        \n",
    "        # retrain and validate on 4 subject splits\n",
    "        epochs = 5\n",
    "        temp_results = [] \n",
    "        for subject_split in range(4):\n",
    "            trial_idx = range(len(subject_X))\n",
    "            train_trials, test_trials = nndata.split_idx(subject_split, 4, trial_idx)\n",
    "\n",
    "            model.load_weights(weights_file % (split))\n",
    "            h = model.fit(\n",
    "                subject_X[train_trials,:], subject_y[train_trials,:], \n",
    "                validation_data=(subject_X[test_trials,:], subject_y[test_trials,:]),\n",
    "                epochs=5, batch_size=2, verbose=0\n",
    "            )\n",
    "            # save best validation accuracy\n",
    "            temp_results.append(np.max(h.history[\"val_acc\"]))\n",
    "        results[current_subject, 1] = np.mean(temp_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input length crossvalidation\n",
    "\n",
    "This code records the average accuracy with respect to the length of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 16\n",
    "SPLITS = 5\n",
    "electrodes = range(64)\n",
    "classes = 3\n",
    "X,y = nndata.load_raw_data(electrodes, num_classes=classes)\n",
    "\n",
    "epochs = 3\n",
    "splits = range(5)\n",
    "steps_per_epoch = np.prod(X.shape[:2]) / BATCH * (1-1./SPLITS)\n",
    "\n",
    "#lengths = np.arange(30, 160, 10) # short, less spacing\n",
    "lengths = np.arange(160, 960, 40) # long, more spacing\n",
    "\n",
    "results = np.zeros((len(lengths), len(splits), 4, epochs))\n",
    "for ii,i in enumerate(splits):\n",
    "    for j, length in enumerate(lengths):\n",
    "        print \"Length %d, SPLIT %d\" % (length, i)\n",
    "        idx = range(len(X))\n",
    "        train_idx, test_idx = nndata.split_idx(i, 5, idx)\n",
    "        model = models.create_raw_model(\n",
    "            nchan=len(electrodes), \n",
    "            nclasses=classes, \n",
    "            trial_length=length\n",
    "        )\n",
    "        \n",
    "        h = models.fit_model(\n",
    "            model, X, y, train_idx, test_idx, \n",
    "            input_length=length, batch_size=BATCH, \n",
    "            steps_per_epoch=steps_per_epoch, epochs=epochs\n",
    "        )\n",
    "        \n",
    "        results[j, ii, :, :] = [ \n",
    "            h.history[\"acc\"], \n",
    "            h.history[\"loss\"], \n",
    "            h.history[\"val_acc\"], \n",
    "            h.history[\"val_loss\"]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online evaluation\n",
    "\n",
    "This code was used to determine the accuracy depending on the position of a data segment relative to the beginning of the MI trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3\n",
    "split = 0\n",
    "\n",
    "step = 20\n",
    "trial_length = 6*160\n",
    "channels =  range(64)\n",
    "# load model\n",
    "model = models.create_raw_model(nchan=len(channels), nclasses=classes, trial_length=input_length)\n",
    "model.load_weights(\"weights-%dcl-%d.hdf5\" % (classes, split))\n",
    "\n",
    "idx = range(len(X))\n",
    "train_idx, test_idx = nndata.split_idx(split, 5, idx)\n",
    "acc = np.zeros((trial_length-input_length)/ step + 1)\n",
    "cert = np.zeros((trial_length-input_length)/ step + 1)\n",
    "falsenull = np.zeros((trial_length-input_length)/ step + 1)\n",
    "for i, off in enumerate(np.arange(0, trial_length-input_length, step)):\n",
    "    print off, \n",
    "    xt,yt = nndata.crossval_test(X, y, test_idx, input_length, fix_offset=off)\n",
    "    yp = model.predict(xt)\n",
    "    cert[i] = yp[yt>0].mean()\n",
    "    acc[i] = sum(yp.argmax(1)==yt.argmax(1)) / float(len(yt))\n",
    "    if classes > 2:\n",
    "        falsenull[i] = sum(yp[yt[:,2]==0].argmax(1)==2) / float(len(yt))\n",
    "        \n",
    "np.save(\"online-certainty.npy\")\n",
    "np.save(\"online-accuracy.npy\")\n",
    "if classes > 2:\n",
    "    np.save(\"online-falsenull.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA of feature detector\n",
    "\n",
    "This code was used to examine and visualize the principal components of the CNN feature detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import nndata\n",
    "import models\n",
    "\n",
    "input_length = 960\n",
    "electrodes = range(64)\n",
    "classes = [2,3,4]\n",
    "split = 0 # select first split as example\n",
    "\n",
    "weights_path = \"weights-%dcl-0.hdf5\" # make sure weights exist %d is a wildcard for number of classes\n",
    "\n",
    "for cls in classes:\n",
    "    X,y = nndata.load_raw_data(electrodes=electrodes, num_classes=cls)\n",
    "    model = models.create_raw_model(nchan=len(electrodes), nclasses=cls, trial_length=input_length, l1=0)\n",
    "    model.load_weights(weights_path % cls)\n",
    "    \n",
    "    # Define tensor function from input to fifth layer\n",
    "    get_layer_output = K.function([model.layers[0].input], [model.layers[4].output])\n",
    "    \n",
    "    idx = range(len(X))\n",
    "    train_idx, test_idx = nndata.split_idx(split, 5, idx)\n",
    "    Xtest,ytest = nndata.crossval_test(X, y, test_idx, input_length)\n",
    "    results = np.zeros((Xtest.shape[0], model.layers[4].output_shape[1]))\n",
    "    for i, xtest in enumerate(Xtest):\n",
    "        results[i,:] = get_layer_output([[xtest]])[0].reshape(-1)\n",
    "    \n",
    "    savemap = dict()\n",
    "    savemap[\"output\"] = results\n",
    "    savemap[\"classes\"] = ytest.argmax(1)\n",
    "    savemap[\"classification\"] = model.predict(Xtest).argmax(1)\n",
    "    # save to file\n",
    "    np.save(\"pca/%dclass.npy\" % cls, savemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure previous section created these files\n",
    "fnames = [\"pca/2class.npy\", \"pca/3class.npy\", \"pca/4class.npy\"]\n",
    "markers = [\"xr\", \"xb\", \"xg\", \"xk\"]\n",
    "plt.figure(figsize=(16,8))\n",
    "for i, name in enumerate(fnames):\n",
    "    m = np.load(name).item()\n",
    "    results = m[\"output\"]\n",
    "    labels = m[\"classes\"]\n",
    "    \n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(results)\n",
    "    Xtrans = pca.transform(results)\n",
    "    \n",
    "    plt.subplot(1,3,i+1)\n",
    "    for j in [0,1,2,3]:\n",
    "        plt.plot(Xtrans[labels==j, 0], Xtrans[labels==j, 1], markers[j])\n",
    "        \n",
    "    plt.xlabel(\"PC 0\")\n",
    "    plt.ylabel(\"PC 1\")\n",
    "    print \"File %s\" % name,\n",
    "    print \"variance explained: %.2f %%\" % (np.sum(pca.explained_variance_ratio_[:2])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
